{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1a819e98",
      "metadata": {},
      "source": [
        "# CoNLL‑2003 Tokenization Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e2afb0",
      "metadata": {},
      "source": [
        "This notebook adapts your IMDb pipeline to **CoNLL‑2003 (NER)** and runs the same tokenizer + TF‑IDF + Logistic Regression evaluation\n",
        "by converting each sentence into text and assigning a **sentence‑level label**.\n",
        "\n",
        "- Uses **Hugging Face `tokenizers`** (BPE / WordPiece / Unigram). No `sentencepiece` required.\n",
        "- Loads CoNLL‑2003 from the dataset's **Parquet branch** (no remote loader script).\n",
        "- Two labeling modes:\n",
        "  - `contains_entity` (binary): 1 if any token has a named-entity tag, else 0.\n",
        "  - `major_entity` (multi‑class): the dominant entity type among {PER, ORG, LOC, MISC}, or NONE.\n",
        "\n",
        "Run cells top‑to‑bottom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a6834356",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Parameters ---\n",
        "# Dataset & labeling\n",
        "DATASET_NAME = \"eriktks/conll2003\"\n",
        "LABEL_MODE   = \"contains_entity\"   # or: \"major_entity\"\n",
        "\n",
        "# Output files for this dataset\n",
        "CORPUS_PATH  = \"corpus/conll2003_corpus.txt\"\n",
        "CSV_PATH     = f\"data/conll2003_{LABEL_MODE}.csv\"\n",
        "\n",
        "# Tokenizer choice (HF only; no SentencePiece required)\n",
        "MODEL_TYPE   = \"unigram\"           # \"unigram\" | \"bpe\" | \"wordpiece\"\n",
        "VOCAB_SIZE   = 10000\n",
        "TOK_PATH     = f\"tokenizers/{MODEL_TYPE}-conll2003-{VOCAB_SIZE//1000}k.json\"\n",
        "\n",
        "# Analysis config\n",
        "TEST_SIZE    = 0.20\n",
        "RANDOM_SEED  = 42\n",
        "TYPO_PROB    = 0.05\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "40848323",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensured directories: data, corpus, tokenizers, results, logs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "for d in [\"data\", \"corpus\", \"tokenizers\", \"results\", \"logs\"]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "print(\"Ensured directories:\", \", \".join(d for d in [\"data\", \"corpus\", \"tokenizers\", \"results\", \"logs\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2905ad2",
      "metadata": {},
      "source": [
        "## Step 1 — Prepare CoNLL‑2003 into corpus + labeled CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a4e2fef0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1 — Prepare CoNLL‑2003 (from Parquet branch; no loader script)\n",
        "from datasets import load_dataset\n",
        "import pandas as pd, os\n",
        "from collections import Counter\n",
        "\n",
        "def _hf_hub_parquet_paths(repo=\"eriktks/conll2003\", revision=\"refs/convert/parquet\"):\n",
        "    # Prefer hf:// protocol; fallback to HTTPS resolve URLs if needed\n",
        "    base_hf = f\"hf://datasets/{repo}@{revision}/conll2003\"\n",
        "    base_https = \"https://huggingface.co/datasets/\" + repo + \"/resolve/\" + revision.replace(\"/\", \"%2F\") + \"/conll2003\"\n",
        "    return {\n",
        "        \"hf\": {\n",
        "            \"train\":      f\"{base_hf}/train/*.parquet\",\n",
        "            \"validation\": f\"{base_hf}/validation/*.parquet\",\n",
        "            \"test\":       f\"{base_hf}/test/*.parquet\",\n",
        "        },\n",
        "        \"https\": {\n",
        "            \"train\":      f\"{base_https}/train/0000.parquet\",\n",
        "            \"validation\": f\"{base_https}/validation/0000.parquet\",\n",
        "            \"test\":       f\"{base_https}/test/0000.parquet\",\n",
        "        },\n",
        "    }\n",
        "\n",
        "def _infer_O_index(seq_iterable):\n",
        "    # Choose the most frequent tag id across ner_tags as 'O' (works for CoNLL-2003)\n",
        "    cnt = Counter()\n",
        "    for seq in seq_iterable:\n",
        "        cnt.update(seq)\n",
        "    o_idx, _ = cnt.most_common(1)[0]\n",
        "    return o_idx\n",
        "\n",
        "def prepare_conll2003(corpus_path: str, csv_path: str, label_mode: str = \"contains_entity\"):\n",
        "    \"\"\"\n",
        "    Build:\n",
        "      - corpus_path: one sentence per line (tokens joined by spaces)\n",
        "      - csv_path: dataframe with columns [text, label] for sentence-level classification\n",
        "\n",
        "    label_mode:\n",
        "      - \"contains_entity\": 1 if any NE tag is not 'O', else 0\n",
        "      - \"major_entity\": one of {NONE, PER, ORG, LOC, MISC} -> ints 0..4 (requires label names)\n",
        "    \"\"\"\n",
        "    paths = _hf_hub_parquet_paths()\n",
        "\n",
        "    # Try hf:// uris (supports wildcards). Fallback to HTTPS resolve URLs.\n",
        "    try:\n",
        "        ds = load_dataset(\n",
        "            \"parquet\",\n",
        "            data_files={\n",
        "                \"train\":      paths[\"hf\"][\"train\"],\n",
        "                \"validation\": paths[\"hf\"][\"validation\"],\n",
        "                \"test\":       paths[\"hf\"][\"test\"],\n",
        "            },\n",
        "        )\n",
        "    except Exception:\n",
        "        ds = load_dataset(\n",
        "            \"parquet\",\n",
        "            data_files={\n",
        "                \"train\":      paths[\"https\"][\"train\"],\n",
        "                \"validation\": paths[\"https\"][\"validation\"],\n",
        "                \"test\":       paths[\"https\"][\"test\"],\n",
        "            },\n",
        "        )\n",
        "\n",
        "    # Get label names if present (Sequence(ClassLabel))\n",
        "    label_names = None\n",
        "    try:\n",
        "        label_feature = ds[\"train\"].features[\"ner_tags\"].feature\n",
        "        label_names = getattr(label_feature, \"names\", None)\n",
        "    except Exception:\n",
        "        label_names = None\n",
        "\n",
        "    # Determine 'O' id\n",
        "    if label_names is not None and \"O\" in label_names:\n",
        "        O_IDX = label_names.index(\"O\")\n",
        "    else:\n",
        "        O_IDX = _infer_O_index(ds[\"train\"][\"ner_tags\"])\n",
        "\n",
        "    rows_text, rows_label = [], []\n",
        "\n",
        "    if label_mode not in {\"contains_entity\", \"major_entity\"}:\n",
        "        raise ValueError(\"label_mode must be 'contains_entity' or 'major_entity'\")\n",
        "\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        tokens_list = ds[split][\"tokens\"]\n",
        "        tags_list   = ds[split][\"ner_tags\"]\n",
        "        for tokens, ner_ids in zip(tokens_list, tags_list):\n",
        "            text = \" \".join(tokens)\n",
        "\n",
        "            if label_mode == \"contains_entity\":\n",
        "                label = 1 if any(tid != O_IDX for tid in ner_ids) else 0\n",
        "            else:  # major_entity\n",
        "                if label_names is None:\n",
        "                    raise RuntimeError(\n",
        "                        \"major_entity requires label names in dataset features. \"\n",
        "                        \"Use label_mode='contains_entity' or another CoNLL version with names.\"\n",
        "                    )\n",
        "                counts = Counter()\n",
        "                for tid in ner_ids:\n",
        "                    tag = label_names[tid]\n",
        "                    if tag != \"O\":\n",
        "                        etype = tag.split(\"-\", 1)[1]  # PER/ORG/LOC/MISC\n",
        "                        counts[etype] += 1\n",
        "                label_text = max(counts, key=counts.get) if counts else \"NONE\"\n",
        "                label_map = {\"NONE\": 0, \"PER\": 1, \"ORG\": 2, \"LOC\": 3, \"MISC\": 4}\n",
        "                label = label_map[label_text]\n",
        "\n",
        "            rows_text.append(text)\n",
        "            rows_label.append(label)\n",
        "\n",
        "    # Save CSV\n",
        "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "    pd.DataFrame({\"text\": rows_text, \"label\": rows_label}).to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Wrote {csv_path} with {len(rows_text)} sentences.\")\n",
        "\n",
        "    # Save raw corpus\n",
        "    os.makedirs(os.path.dirname(corpus_path), exist_ok=True)\n",
        "    with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for t in rows_text:\n",
        "            f.write(t + \"\\n\")\n",
        "    print(f\"Wrote {corpus_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9ac9917c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote data/conll2003_contains_entity.csv with 20744 sentences.\n",
            "Wrote corpus/conll2003_corpus.txt.\n"
          ]
        }
      ],
      "source": [
        "# Run Step 1\n",
        "prepare_conll2003(CORPUS_PATH, CSV_PATH, label_mode=LABEL_MODE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf5cc089",
      "metadata": {},
      "source": [
        "## Step 2 — Train a tokenizer (HF `tokenizers`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dd0cb82f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train tokenizer (HF `tokenizers`: BPE / WordPiece / Unigram)\n",
        "import os\n",
        "from tokenizers import Tokenizer as HFTokenizer\n",
        "from tokenizers.models import BPE, WordPiece, Unigram\n",
        "from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence as NormalizerSequence\n",
        "\n",
        "def train_tokenizer_hf(model_type: str, corpus_file: str, output_path: str, vocab_size: int = 10000):\n",
        "    if not os.path.exists(corpus_file):\n",
        "        raise FileNotFoundError(f\"Corpus not found: {corpus_file}\")\n",
        "\n",
        "    mt = model_type.lower()\n",
        "    if mt not in {\"bpe\", \"wordpiece\", \"unigram\"}:\n",
        "        raise ValueError(\"model_type must be one of: 'bpe', 'wordpiece', 'unigram'\")\n",
        "\n",
        "    if mt == \"bpe\":\n",
        "        model = BPE(unk_token=\"[UNK]\")\n",
        "        trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "    elif mt == \"wordpiece\":\n",
        "        model = WordPiece(unk_token=\"[UNK]\")\n",
        "        trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "    else:  # unigram\n",
        "        model = Unigram()\n",
        "        trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "    tok = HFTokenizer(model)\n",
        "    # Normalization similar to earlier project\n",
        "    tok.normalizer = NormalizerSequence([NFD(), Lowercase(), StripAccents()])\n",
        "    tok.pre_tokenizer = Whitespace()\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
        "    tok.train([corpus_file], trainer)\n",
        "    tok.save(output_path)\n",
        "    print(f\"Saved tokenizer to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "56b51405",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved tokenizer to: tokenizers/unigram-conll2003-10k.json\n",
            "Tokenizer saved to: tokenizers/unigram-conll2003-10k.json\n"
          ]
        }
      ],
      "source": [
        "# Run Step 2\n",
        "train_tokenizer_hf(MODEL_TYPE, CORPUS_PATH, TOK_PATH, VOCAB_SIZE)\n",
        "print(\"Tokenizer saved to:\", TOK_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd37a2fb",
      "metadata": {},
      "source": [
        "## Step 3 — Run the analysis (Zipf + TF‑IDF + Logistic Regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a7fa018",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis — Zipf + TF‑IDF + Logistic Regression (SentencePiece optional for .model files)\n",
        "import os, re, random, string, csv\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Couldn't install sentencepiece on my machine - used only if a .model tokenizer is passed\n",
        "try:\n",
        "    import sentencepiece as spm\n",
        "except Exception:\n",
        "    spm = None\n",
        "\n",
        "from tokenizers import Tokenizer as HFTokenizer\n",
        "\n",
        "def sanitize_name(path: str) -> str:\n",
        "    base = os.path.basename(path)\n",
        "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", os.path.splitext(base)[0])\n",
        "\n",
        "def load_tokenizer_any(tokenizer_path: str):\n",
        "    \"\"\"\n",
        "    Load either an HF Tokenizer (.json) or a SentencePiece model (.model).\n",
        "    Returns (encode_to_tokens_fn, vocab_tokens, kind) where kind in {\"hf\",\"sp\"}.\n",
        "    \"\"\"\n",
        "    if tokenizer_path.endswith(\".model\"):\n",
        "        if spm is None:\n",
        "            raise ImportError(\"This is a SentencePiece .model. Install `sentencepiece` or use an HF .json tokenizer.\")\n",
        "        sp = spm.SentencePieceProcessor(model_file=tokenizer_path)\n",
        "        vocab_tokens = [sp.id_to_piece(i) for i in range(sp.vocab_size())]\n",
        "        def enc(text: str): return sp.encode(text, out_type=str)\n",
        "        return enc, vocab_tokens, \"sp\"\n",
        "\n",
        "    # HF tokenizer (.json) — no sentencepiece needed\n",
        "    tok = HFTokenizer.from_file(tokenizer_path)\n",
        "    vocab = tok.get_vocab()  # dict token -> id\n",
        "    vocab_tokens = [t for t, i in sorted(vocab.items(), key=lambda kv: kv[1])]\n",
        "    def enc(text: str): return tok.encode(text).tokens\n",
        "    return enc, vocab_tokens, \"hf\"\n",
        "\n",
        "def make_results_dir():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "def tokens_for_texts(encode_fn, texts):\n",
        "    # Return pre-tokenized strings (space-delimited) for TF-IDF\n",
        "    return [\" \".join(encode_fn(t)) for t in texts]\n",
        "\n",
        "def add_typos(s: str, p: float = 0.05, letters: str = string.ascii_lowercase):\n",
        "    # ~5% random character substitutions on alphabetic chars\n",
        "    out = []\n",
        "    for ch in s:\n",
        "        if ch.isalpha() and random.random() < p: out.append(random.choice(letters))\n",
        "        else: out.append(ch)\n",
        "    return \"\".join(out)\n",
        "\n",
        "def plot_zipf(token_lists, save_path: str, title: str):\n",
        "    # token_lists: list[list[str]]\n",
        "    freq = Counter(t for toks in token_lists for t in toks)\n",
        "    if not freq:\n",
        "        print(\"No tokens to plot Zipf.\"); return\n",
        "    counts = np.array(sorted(freq.values(), reverse=True), dtype=np.float64)\n",
        "    ranks  = np.arange(1, len(counts) + 1, dtype=np.float64)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.loglog(ranks, counts)\n",
        "    plt.xlabel(\"Rank\"); plt.ylabel(\"Frequency\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout(); plt.savefig(save_path, dpi=160); plt.close()\n",
        "    print(f\"Saved Zipf plot -> {save_path}\")\n",
        "\n",
        "def run_full_analysis(tokenizer_path: str, data_path: str,\n",
        "                      test_size=0.2, random_state=42, typo_prob=0.05):\n",
        "    make_results_dir()\n",
        "    random.seed(random_state)\n",
        "\n",
        "    name        = sanitize_name(tokenizer_path)\n",
        "    report_txt  = os.path.join(\"results\", f\"classification_report_{name}.txt\")\n",
        "    zipf_png    = os.path.join(\"results\", f\"zipf_plot_{name}.png\")\n",
        "    results_csv = os.path.join(\"results\", \"full_results.csv\")\n",
        "\n",
        "    encode_fn, vocab_tokens, kind = load_tokenizer_any(tokenizer_path)\n",
        "    print(f\"Loaded tokenizer kind={kind}, vocab size={len(vocab_tokens)}\")\n",
        "\n",
        "    # Load labeled data\n",
        "    df = pd.read_csv(data_path)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df[\"text\"].tolist(), df[\"label\"].tolist(),\n",
        "        test_size=test_size, random_state=random_state, stratify=df[\"label\"]\n",
        "    )\n",
        "\n",
        "    # Zipf on training tokens\n",
        "    tok_lists_for_zipf = [encode_fn(t) for t in X_train]\n",
        "    plot_zipf(tok_lists_for_zipf, zipf_png, f\"Zipf — {name} (|V|={len(vocab_tokens)})\")\n",
        "\n",
        "    # Pre-tokenize (space-delimited strings)\n",
        "    X_train_tok = tokens_for_texts(encode_fn, X_train)\n",
        "    X_test_tok  = tokens_for_texts(encode_fn, X_test)\n",
        "\n",
        "    # Noisy test set: inject character typos BEFORE re-tokenizing\n",
        "    X_test_noisy_txt = [add_typos(t, p=typo_prob) for t in X_test]\n",
        "    X_test_noisy_tok = tokens_for_texts(encode_fn, X_test_noisy_txt)\n",
        "\n",
        "    # TF-IDF with a FIXED vocabulary = tokenizer's tokens\n",
        "    vocab_map = {tok: i for i, tok in enumerate(vocab_tokens)}\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        vocabulary=vocab_map,\n",
        "        tokenizer=str.split,\n",
        "        preprocessor=None,\n",
        "        token_pattern=None,\n",
        "        lowercase=False,\n",
        "        ngram_range=(1, 1),\n",
        "    )\n",
        "\n",
        "    Xtr = vectorizer.fit_transform(X_train_tok)\n",
        "    Xte = vectorizer.transform(X_test_tok)\n",
        "    Xte_noisy = vectorizer.transform(X_test_noisy_tok)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(Xtr, y_train)\n",
        "\n",
        "    y_pred_clean = clf.predict(Xte)\n",
        "    y_pred_noisy = clf.predict(Xte_noisy)\n",
        "\n",
        "    clean_acc = accuracy_score(y_test, y_pred_clean)\n",
        "    noisy_acc = accuracy_score(y_test, y_pred_noisy)\n",
        "    drop_pct  = max(0.0, (clean_acc - noisy_acc) * 100.0)\n",
        "\n",
        "    # Save detailed reports\n",
        "    rep_clean = classification_report(y_test, y_pred_clean, digits=4)\n",
        "    rep_noisy = classification_report(y_test, y_pred_noisy, digits=4)\n",
        "    with open(report_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"Tokenizer: {name}\\nVocab size: {len(vocab_tokens)}\\n\\n\")\n",
        "        f.write(\"=== CLEAN ===\\n\" + rep_clean + \"\\n\\n\")\n",
        "        f.write(\"=== NOISY ===\\n\" + rep_noisy + \"\\n\")\n",
        "    print(f\"Wrote report -> {report_txt}\")\n",
        "\n",
        "    # Append summary CSV\n",
        "    write_hdr = not os.path.exists(results_csv)\n",
        "    with open(results_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        if write_hdr:\n",
        "            w.writerow([\"Tokenizer\", \"Vocab\", \"Clean Acc\", \"Noisy Acc\", \"Drop (%)\"])\n",
        "        w.writerow([name, len(vocab_tokens),\n",
        "                    round(clean_acc, 4), round(noisy_acc, 4), round(drop_pct, 2)])\n",
        "    print(f\"Appended results -> {results_csv}\")\n",
        "    print({\"Tokenizer\": name, \"Vocab\": len(vocab_tokens), \"Clean Acc\": clean_acc,\n",
        "           \"Noisy Acc\": noisy_acc, \"Drop (%)\": drop_pct})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4c09e1b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer kind=hf, vocab size=9374\n",
            "Saved Zipf plot -> results\\zipf_plot_unigram-conll2003-10k.png\n",
            "Wrote report -> results\\classification_report_unigram-conll2003-10k.txt\n",
            "Appended results -> results\\full_results.csv\n",
            "{'Tokenizer': 'unigram-conll2003-10k', 'Vocab': 9374, 'Clean Acc': 0.8864786695589298, 'Noisy Acc': 0.8532176428054953, 'Drop (%)': 3.3261026753434564}\n"
          ]
        }
      ],
      "source": [
        "# Run Step 3\n",
        "run_full_analysis(\n",
        "    tokenizer_path=TOK_PATH,   # .json from Step 2\n",
        "    data_path=CSV_PATH,        # CSV from Step 1\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_SEED,\n",
        "    typo_prob=TYPO_PROB\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66ae88e",
      "metadata": {},
      "source": [
        "## Step 4 — Visualize aggregate results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e5af86c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved results/final_performance_chart.png\n",
            "Saved results/robustness_drop_chart.png\n"
          ]
        }
      ],
      "source": [
        "# Visualize final charts\n",
        "import os, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_results(csv_path=\"results/full_results.csv\"):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"No results CSV found at {csv_path}. Run the analysis first.\"); return\n",
        "    df = pd.read_csv(csv_path).sort_values(\"Clean Acc\", ascending=False)\n",
        "\n",
        "    # Bar: Clean vs Noisy\n",
        "    plt.figure()\n",
        "    idx = range(len(df))\n",
        "    plt.bar([i - 0.2 for i in idx], df[\"Clean Acc\"], width=0.4, label=\"Clean\")\n",
        "    plt.bar([i + 0.2 for i in idx], df[\"Noisy Acc\"], width=0.4, label=\"Noisy\")\n",
        "    plt.xticks(list(idx), df[\"Tokenizer\"], rotation=30, ha=\"right\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"CoNLL-2003 — Accuracy (Clean vs Noisy)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    out1 = \"results/final_performance_chart.png\"\n",
        "    plt.savefig(out1, dpi=160); plt.close()\n",
        "    print(f\"Saved {out1}\")\n",
        "\n",
        "    # Bar: Drop (%)\n",
        "    plt.figure()\n",
        "    plt.bar(list(idx), df[\"Drop (%)\"])\n",
        "    plt.xticks(list(idx), df[\"Tokenizer\"], rotation=30, ha=\"right\")\n",
        "    plt.ylabel(\"Drop (%)\")\n",
        "    plt.title(\"CoNLL-2003 — Robustness (Accuracy Drop)\")\n",
        "    plt.tight_layout()\n",
        "    out2 = \"results/robustness_drop_chart.png\"\n",
        "    plt.savefig(out2, dpi=160); plt.close()\n",
        "    print(f\"Saved {out2}\")\n",
        "\n",
        "# Run Step 4\n",
        "visualize_results()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef67c81",
      "metadata": {},
      "source": [
        "## Compare Unigram, BPE, WordPiece quickly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f9161203",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer kind=hf, vocab size=9374\n",
            "Saved Zipf plot -> results\\zipf_plot_unigram-conll2003-10k.png\n",
            "Wrote report -> results\\classification_report_unigram-conll2003-10k.txt\n",
            "Appended results -> results\\full_results.csv\n",
            "{'Tokenizer': 'unigram-conll2003-10k', 'Vocab': 9374, 'Clean Acc': 0.8864786695589298, 'Noisy Acc': 0.8532176428054953, 'Drop (%)': 3.3261026753434564}\n",
            "Saved tokenizer to: tokenizers/bpe-conll2003-10k.json\n",
            "Loaded tokenizer kind=hf, vocab size=10000\n",
            "Saved Zipf plot -> results\\zipf_plot_bpe-conll2003-10k.png\n",
            "Wrote report -> results\\classification_report_bpe-conll2003-10k.txt\n",
            "Appended results -> results\\full_results.csv\n",
            "{'Tokenizer': 'bpe-conll2003-10k', 'Vocab': 10000, 'Clean Acc': 0.8768377922390937, 'Noisy Acc': 0.8479151602795855, 'Drop (%)': 2.8922631959508283}\n",
            "Saved tokenizer to: tokenizers/wordpiece-conll2003-10k.json\n",
            "Loaded tokenizer kind=hf, vocab size=10000\n",
            "Saved Zipf plot -> results\\zipf_plot_wordpiece-conll2003-10k.png\n",
            "Wrote report -> results\\classification_report_wordpiece-conll2003-10k.txt\n",
            "Appended results -> results\\full_results.csv\n",
            "{'Tokenizer': 'wordpiece-conll2003-10k', 'Vocab': 10000, 'Clean Acc': 0.8768377922390937, 'Noisy Acc': 0.8455049409496265, 'Drop (%)': 3.133285128946728}\n",
            "Saved results/final_performance_chart.png\n",
            "Saved results/robustness_drop_chart.png\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "for mt in [\"unigram\", \"bpe\", \"wordpiece\"]:\n",
        "    tok_path = f\"tokenizers/{mt}-conll2003-{VOCAB_SIZE//1000}k.json\"\n",
        "    if not os.path.exists(tok_path):\n",
        "        train_tokenizer_hf(mt, CORPUS_PATH, tok_path, VOCAB_SIZE)\n",
        "    run_full_analysis(\n",
        "        tokenizer_path=tok_path,\n",
        "        data_path=CSV_PATH,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=RANDOM_SEED,\n",
        "        typo_prob=TYPO_PROB\n",
        "    )\n",
        "visualize_results()\n"
      ]
    }
  ],
  "metadata": {
    "description": "HF-only tokenization pipeline on CoNLL-2003 (Parquet); SentencePiece is optional.",
    "kernelspec": {
      "display_name": "Python (your_env_name)",
      "language": "python",
      "name": "your_env_name"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
